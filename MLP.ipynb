{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classification by training on trajectory\n",
    "\n",
    "<br> This notebook is adopted from the work: https://pubs.acs.org/doi/10.1021/acs.jpclett.1c01494\n",
    "<br> Git repo: https://github.com/zzhang624/ML-SARS-CoV-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 662)\n",
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "(15000, 662)\n",
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "crRNA = np.load(\"../New_model_2/crRNA_dist_feature.npy\")[:15000]\n",
    "tgRNA = np.load(\"../New_model_2/tgRNA_dist_feature.npy\")[:15000]\n",
    "\n",
    "# crRNA = np.concatenate((crRNA_f1, crRNA_f2, crRNA_f3, crRNA_f4,crRNA_f5), axis=1)\n",
    "print(crRNA.shape)\n",
    "print(np.where(crRNA[:,:] == 0))\n",
    "\n",
    "print(tgRNA.shape)\n",
    "print(np.where(tgRNA[:,:] == 0))\n",
    "\n",
    "# model5_training = np.load(\"model-5-HEPN1I-HEPN2/model-5-final-training-data.npy\")\n",
    "# print(model5_training.shape)\n",
    "# print(model5_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(np.concatenate((crRNA,tgRNA),axis=0))\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of features before drop: 662\n"
     ]
    }
   ],
   "source": [
    "#inverse distance\n",
    "df=1./df\n",
    "\n",
    "# #add label\n",
    "df['state'] = 'state'\n",
    "df.iloc[:15000,-1] = 1\n",
    "df.iloc[15000:,-1] = 2\n",
    "\n",
    "#scaled/normalized by each feature\n",
    "df_scaled = df.copy()\n",
    "X = df_scaled.iloc[:,:-1]\n",
    "X = (X-X.mean())/X.std()\n",
    "# print(X.mean()[22], X.std()[22])\n",
    "df_scaled.update(X)\n",
    "df_scaled\n",
    "#features\n",
    "features_pre = df_scaled.iloc[:,:-1]\n",
    "#features\n",
    "features_pre = df_scaled.iloc[:,:-1]\n",
    "\n",
    "print('# of features before drop:', features_pre.shape[1])\n",
    "\n",
    "#create correlation matrix\n",
    "corr_matrix_before = features_pre.corr().abs()\n",
    "\n",
    "#set a cutoff threshold (usually 0.9 or 0.85)\n",
    "cut_th = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# features_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"model-10-final-training-scaled-data.npy\",features_pre.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import scikit learn packages\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.metrics import (recall_score,accuracy_score,confusion_matrix, f1_score, precision_score, \n",
    "                             auc,roc_auc_score,roc_curve, precision_recall_curve,classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6n/3nqlb7m93wg2vxy4dg00ggzh0000gn/T/ipykernel_47964/255855387.py:6: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  upper = corr_matrix_before.where(np.triu(np.ones(corr_matrix_before.shape), k=1).astype(np.bool))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped columns: 42\n",
      "(30000, 620) (30000,)\n",
      "[[ 1.90008464  1.97686647  0.67368321 ...  0.28215796  1.55231471\n",
      "  -0.3265295 ]\n",
      " [-0.33840144  0.0619965  -0.84642334 ...  0.93215241  0.73220886\n",
      "  -0.10403966]\n",
      " [-0.34974641  0.12145749 -0.52864777 ... -0.19206827  1.06780244\n",
      "  -0.61708815]\n",
      " ...\n",
      " [-1.24389149  1.75802147  0.80838026 ... -1.65977353  0.96473615\n",
      "  -0.99285142]\n",
      " [ 0.44826066 -1.36922101 -1.09743236 ...  1.68634421 -1.10581234\n",
      "  -1.02371726]\n",
      " [-0.41218783 -0.58829468 -1.08572197 ...  0.0797455  -1.02694162\n",
      "  -0.43441097]]\n"
     ]
    }
   ],
   "source": [
    "#### Model-10 ####\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#keeping the upper (right) triangular matrix without the main diagonal\n",
    "upper = corr_matrix_before.where(np.triu(np.ones(corr_matrix_before.shape), k=1).astype(np.bool))\n",
    "\n",
    "#removed a column (i.e., feature), if any element(s) in the column of the upper triangular matrix is/are \n",
    "#larger than the threshold value\n",
    "cut_th = 0.95\n",
    "to_drop = [column for column in upper.columns if (any(upper[column] > cut_th) and column not in [6271, 6276, 6758, 6762])]\n",
    "print(\"dropped columns:\", len(to_drop))\n",
    "df_dropped = df_scaled.drop(columns = to_drop)\n",
    "\n",
    "\n",
    "X = df_dropped.iloc[:,:-1]\n",
    "y = df_dropped['state']\n",
    "\n",
    "if type(X) is not np.ndarray:\n",
    "    X, y = X.to_numpy(),y.to_numpy()\n",
    "else:\n",
    "    print('X,y already converted to ndarray')\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "# # print(y.shape)\n",
    "#convert labels to one hot code for MLP\n",
    "one_hot_y = y[:,None]==np.array([1,2])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,one_hot_y,test_size = 0.20, random_state =4) \n",
    "X1 = X[:15000,]\n",
    "X2 = X[15000:,]\n",
    "y1 = one_hot_y[:15000,]  ## for MLP\n",
    "y2 = one_hot_y[15000:,] ## for MLP\n",
    "\n",
    "# print(X1.shape, X2.shape, y1.shape, y2.shape)\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1,y1,test_size = 0.20, random_state =1) \n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2,y2,test_size = 0.20, random_state =1) \n",
    "\n",
    "X_train = np.concatenate((X1_train,X2_train),axis=0)\n",
    "y_train = np.concatenate((y1_train,y2_train),axis=0)\n",
    "X_test = np.concatenate((X1_test,X2_test),axis=0)\n",
    "y_test = np.concatenate((y1_test,y2_test),axis=0)\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_y = y[:,None]==np.array([1,2])\n",
    "# print(one_hot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"model10-after_dropped-training-scaled-data.npy\",df_dropped.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 91) (24000, 2) (6000, 91) (6000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features selected to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121, 121)\n",
      "keep columns: 91\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39, 41, 42, 43, 44, 45, 47, 48, 50, 52, 53, 54, 55, 56, 58, 59, 63, 64, 66, 67, 68, 69, 70, 72, 74, 75, 77, 78, 80, 81, 83, 85, 88, 89, 92, 96, 97, 99, 100, 101, 102, 103, 105, 107, 108, 109, 110, 111, 113, 114, 116, 118, 119, 120]\n"
     ]
    }
   ],
   "source": [
    "## Will use this to drop columns in blind datasets\n",
    "keep = [column for column in df_dropped.iloc[:,:-1].columns]\n",
    "print(upper.shape)\n",
    "print(\"keep columns:\", len(keep))\n",
    "print(keep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.35953303\n",
      "Iteration 2, loss = 0.04526322\n",
      "Iteration 3, loss = 0.01728222\n",
      "Iteration 4, loss = 0.00917314\n",
      "Iteration 5, loss = 0.00571747\n",
      "Iteration 6, loss = 0.00392574\n",
      "Iteration 7, loss = 0.00287536\n",
      "Iteration 8, loss = 0.00219750\n",
      "Iteration 9, loss = 0.00173726\n",
      "Iteration 10, loss = 0.00140359\n",
      "Iteration 11, loss = 0.00115874\n",
      "Iteration 12, loss = 0.00096873\n",
      "Iteration 13, loss = 0.00082394\n",
      "Iteration 14, loss = 0.00070750\n",
      "Iteration 15, loss = 0.00061229\n",
      "Iteration 16, loss = 0.00053415\n",
      "Iteration 17, loss = 0.00047040\n",
      "Iteration 18, loss = 0.00041678\n",
      "Iteration 19, loss = 0.00037056\n",
      "Iteration 20, loss = 0.00033241\n",
      "Iteration 21, loss = 0.00029828\n",
      "Iteration 22, loss = 0.00026920\n",
      "Iteration 23, loss = 0.00024405\n",
      "Iteration 24, loss = 0.00022195\n",
      "Iteration 25, loss = 0.00020255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MLPClassifier(max_iter=500), n_jobs=-1,\n",
       "             param_grid={'activation': ['relu'],\n",
       "                         'hidden_layer_sizes': [(5,), 10, (20,)],\n",
       "                         'learning_rate': ['adaptive'], 'solver': ['adam'],\n",
       "                         'verbose': ['True']})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # clf=MLP.fit(X_train, y_train)\n",
    "### For MLP Classifier\n",
    "mlp_gs = MLPClassifier(max_iter=500)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(5,),(10),(20,)], ## (root-over (input layer size * output layer size))\n",
    "#     'hidden_layer_sizes': [(30), (50,),(70,)], ## (root-over (input layer size * output layer size))\n",
    "#     'hidden_layer_sizes': [(20,)],\n",
    "#     'activation': ['relu', 'tanh'],\n",
    "    'activation': ['relu'],\n",
    "# #     'activation': ['tanh', 'relu'],\n",
    "# #     'activation': ['logistic','tanh'],\n",
    "# #     'solver': ['sgd', 'adam'],\n",
    "    'solver': ['adam'],\n",
    "# #     'learning_rate': ['constant','adaptive'],\n",
    "    'learning_rate': ['adaptive'],\n",
    "    'verbose': ['True'],\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train, y_train) # X is train samples and y is the corresponding labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Best parameters found:\n",
      " {'activation': 'relu', 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'adam', 'verbose': 'True'}\n",
      "1.000 (+/-0.000) for {'activation': 'relu', 'hidden_layer_sizes': (5,), 'learning_rate': 'adaptive', 'solver': 'adam', 'verbose': 'True'}\n",
      "1.000 (+/-0.000) for {'activation': 'relu', 'hidden_layer_sizes': 10, 'learning_rate': 'adaptive', 'solver': 'adam', 'verbose': 'True'}\n",
      "1.000 (+/-0.000) for {'activation': 'relu', 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'adam', 'verbose': 'True'}\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(X_test, y_test))\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    " \n",
    "# # # Save the model as a pickle in a file\n",
    "# joblib.dump(clf, 'cas13_trainedModel9_HEPN1-2_MLP.pkl') #''activation': 'relu', 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'adam', 'verbose': 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP classifier parameters, hyperparameters were tuned for an accuracy of 1\n",
    "MLP = MLPClassifier(hidden_layer_sizes = (20), activation = 'relu', solver = 'adam', max_iter= 500) ##Model=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=MLP.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting y for X_val\n",
    "y_true, y_pred = y_test , clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier : 1.0\n"
     ]
    }
   ],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "   diagonal_sum = confusion_matrix.trace()\n",
    "   sum_of_all_elements = confusion_matrix.sum()\n",
    "   return diagonal_sum / sum_of_all_elements\n",
    "\n",
    "#Importing Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#Comparing the predictions against the actual observations in y_val\n",
    "cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\n",
    "#Printing the accuracy\n",
    "print(\"Accuracy of MLPClassifier :\", accuracy(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainedModel10_MLP.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    " \n",
    "# # # Save the model as a pickle in a file\n",
    "joblib.dump(clf, 'trainedModel10_MLP.pkl') #''activation': 'relu', 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'adam', 'verbose': 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load new data to see whether the model can predict the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Load the model from the file\n",
    "clf_from_joblib = joblib.load('trainedModel10_MLP.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 92)\n",
      "             0         1         2         3         4         5         6   \\\n",
      "0     -0.629672 -0.323976 -0.346787   0.29724  0.425553  -0.46048 -0.819467   \n",
      "1     -0.874482 -1.059826 -0.915018 -0.340862 -0.113874 -0.913905 -1.287489   \n",
      "2     -0.908217 -1.097013 -1.074829 -0.443913 -0.224113 -1.180518  -1.73408   \n",
      "3      -0.90639 -0.934969 -0.844596 -0.222192 -0.125362 -1.064765 -1.247103   \n",
      "4     -0.886505 -0.430737 -0.820862 -0.457413 -0.976199 -1.223934  -1.42749   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "29995  0.573253  1.109363  1.052025 -0.833726  0.926996  1.534224  1.702952   \n",
      "29996  0.195631  1.009605  1.404647 -0.813512  0.829126  1.441796  1.108841   \n",
      "29997   0.07855  0.667228  0.385573 -0.569547  0.629942  0.680768  0.228927   \n",
      "29998  0.048143  0.519618  0.571561 -0.793605  0.606106  1.462611  0.684186   \n",
      "29999 -0.023088  0.456379 -0.148084 -1.162912  0.514142  0.377932 -0.009596   \n",
      "\n",
      "             7         8         9   ...        81        82        83  \\\n",
      "0     -0.715903  0.019788 -1.328381  ... -1.034289 -0.654064 -0.800553   \n",
      "1     -1.181631 -0.353196 -1.387689  ... -0.937219 -0.311475 -0.919988   \n",
      "2     -1.643247 -0.694046  -1.58844  ... -0.395305  0.311032 -0.746128   \n",
      "3     -1.092614 -0.350915 -0.683186  ... -0.115689  0.624148 -0.859718   \n",
      "4     -1.307285 -0.829494 -1.320422  ... -0.808899 -0.409113 -0.940134   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "29995  2.037485  1.535513  0.723971  ... -0.314885  0.603473  -0.12088   \n",
      "29996  1.024418  1.401055  0.567213  ... -0.126927   0.82134 -0.470213   \n",
      "29997  0.235729  1.263952 -0.612323  ...  0.129868  1.028911 -0.267306   \n",
      "29998  0.909092  1.676147 -0.130782  ...  0.007493  0.965752 -0.247809   \n",
      "29999  0.085002  1.185203 -0.491116  ...  -0.02302  0.917406  0.142474   \n",
      "\n",
      "             84        85        86        87        88        89        90  \n",
      "0     -0.887302 -0.884365 -0.631089 -0.911412 -0.672042 -1.078845 -0.962066  \n",
      "1     -0.829147 -0.916541 -0.559434 -0.895954 -0.701549 -0.987365 -1.103453  \n",
      "2     -0.572035 -0.744707  -0.23355 -0.680337 -0.576749 -0.773043 -0.715977  \n",
      "3     -0.768288 -0.746956 -0.535168  -0.67454 -0.466963 -0.632518 -0.446177  \n",
      "4     -0.820606 -1.239722  -0.96366 -0.881256 -1.292868 -0.946481 -1.230019  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "29995 -0.424816  -0.19928 -0.210407 -0.327242   0.48142  -0.62537   0.11574  \n",
      "29996 -0.407085  0.004591  -0.20791 -0.233507  0.764831 -0.362415  0.432085  \n",
      "29997 -0.095908  0.249381  0.026081 -0.062051  1.155693  -0.29415  0.588548  \n",
      "29998 -0.261397  0.271896  0.021992  0.023501  1.160841  -0.27512  0.576716  \n",
      "29999 -0.110107  0.132427  0.294719 -0.196121  1.247772 -0.406101  0.526403  \n",
      "\n",
      "[30000 rows x 91 columns]\n"
     ]
    }
   ],
   "source": [
    "## For normalization of the blind dataset\n",
    "import numpy as np\n",
    "X1 = np.load(\"model10-after_dropped-training-scaled-data.npy\", allow_pickle=True)\n",
    "print(X1.shape)\n",
    "# # print(np.arange(X.shape[1]))\n",
    "X = pd.DataFrame(X1[:,:-1])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 121)\n",
      "[[0.04645908 0.04931578 0.05198285 ... 0.05055775 0.04990144 0.04021748]\n",
      " [0.04599679 0.04929151 0.04844708 ... 0.0528009  0.05077157 0.04151761]\n",
      " [0.04573089 0.04904905 0.04831232 ... 0.05321961 0.0487054  0.03986515]\n",
      " ...\n",
      " [0.04478709 0.0485464  0.04690447 ... 0.04253328 0.03917655 0.03396482]\n",
      " [0.04312899 0.04682549 0.04555294 ... 0.04179424 0.0397465  0.0334677 ]\n",
      " [0.04268947 0.04283015 0.04445565 ... 0.04000773 0.03901433 0.03299315]]\n"
     ]
    }
   ],
   "source": [
    "a = np.load(\"../load_new_system_dist_feature.npy\")\n",
    "a_temp = 1/a\n",
    "\n",
    "print(a_temp.shape)\n",
    "print(a_temp) # inverse distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "drop_ca = [i for i in np.arange(tgRNA.shape[1]) if i not in keep]\n",
    "print(len(keep))\n",
    "print(len(drop_ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 91)\n",
      "0.04645907944682835\n",
      "[[0.04645908 0.04931578 0.05198285 ... 0.05055775 0.04990144 0.04021748]\n",
      " [0.04599679 0.04929151 0.04844708 ... 0.0528009  0.05077157 0.04151761]\n",
      " [0.04573089 0.04904905 0.04831232 ... 0.05321961 0.0487054  0.03986515]\n",
      " ...\n",
      " [0.04478709 0.0485464  0.04690447 ... 0.04253328 0.03917655 0.03396482]\n",
      " [0.04312899 0.04682549 0.04555294 ... 0.04179424 0.0397465  0.0334677 ]\n",
      " [0.04268947 0.04283015 0.04445565 ... 0.04000773 0.03901433 0.03299315]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df3=pd.DataFrame(a_temp)\n",
    "\n",
    "# df4 = df3.drop(df3.columns[drop_ca],axis = 1)\n",
    "df4 = df3.drop(columns = drop_ca)\n",
    "x1 = df4.to_numpy()\n",
    "print(x1.shape)\n",
    "# print(df4)\n",
    "\n",
    "# #scaled/normalized by each feature\n",
    "\n",
    "for i in (np.arange(X.shape[1])):\n",
    "    x1[:,i] = (x1[:,i] - X[i].mean(axis=0))/X[i].std(axis=0)\n",
    "print(x1[0,0])\n",
    "print(x1)\n",
    "# df1_scaled= (df1_scaled-X2.mean())/X2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the loaded model to make predictions\n",
    "pred = clf_from_joblib.predict(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 2)\n",
      "0 1500 0\n",
      "% of active-like state: 1.0\n",
      "% of inactive-like state: 0.0\n",
      "% of undecided states: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(pred.shape)\n",
    "c = 0\n",
    "d = 0\n",
    "e = 0\n",
    "f = []\n",
    "for i, j in enumerate(pred.argmax(axis=1)):\n",
    "    if ((pred[i,0] == 1) & (pred[i,1] == 0)):\n",
    "        c += 1\n",
    "        f.append(i)\n",
    "    if ((pred[i,0] == 0) & (pred[i,1] == 1)):\n",
    "        d += 1\n",
    "    if ((pred[i,0] == 0) & (pred[i,1] == 0)):\n",
    "        e += 1\n",
    "    if ((pred[i,0] == 1) & (pred[i,1] == 1)):\n",
    "        e += 1\n",
    "    \n",
    "print(c,d,e)\n",
    "print(\"% of tgRNA-like state:\", d/pred.shape[0])\n",
    "print(\"% of crRNA-like state:\", c/pred.shape[0])\n",
    "print(\"% of undecided states:\", e/pred.shape[0])\n",
    "# print(len(f))\n",
    "# print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20841629 0.69930449]\n",
      " [0.21985708 0.69776183]\n",
      " [0.22248637 0.69862823]\n",
      " [0.23666826 0.68403403]\n",
      " [0.23330607 0.69032897]\n",
      " [0.23542974 0.68441936]\n",
      " [0.22054212 0.70115378]\n",
      " [0.23405192 0.68905447]\n",
      " [0.22453735 0.69959222]\n",
      " [0.22387368 0.69549661]]\n"
     ]
    }
   ],
   "source": [
    "# print(atgRNA[80:85,:])\n",
    "# print(clf_from_joblib.predict(X1[4:50,:]))\n",
    "prob_results = clf_from_joblib.predict_proba(x1[100:110])\n",
    "print(prob_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
